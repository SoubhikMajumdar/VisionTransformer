{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoubhikMajumdar/VisionTransformer/blob/main/ViTnotebookv3_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IJww0E-JtNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafcc0fe-19ac-4378-bcb2-91556f380bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install einops torchvision --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLYrag9kJ0G_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, emb_dim=128):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.proj(x)                         # [B, emb_dim, H/P, W/P]\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c') # [B, N, D]\n",
        "        cls_token = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)     # [B, N+1, D]\n",
        "        return x + self.pos_embed\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, emb_dim=128, num_heads=4, mlp_dim=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_dim, emb_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, emb_dim=128, depth=9,\n",
        "                 num_heads=4, mlp_dim=256, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.embed = PatchEmbedding(img_size, patch_size, in_channels=3, emb_dim=emb_dim)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerEncoder(emb_dim, num_heads, mlp_dim) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "        self.cls_head = nn.Linear(emb_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]  # First token for classification\n",
        "        return self.cls_head(cls_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWsMELqgJ0DY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13c7884-7778-4373-f346-ba75b17c6690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 57.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.64MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.4MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.66MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(32),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TxZjbFdJ0A5",
        "outputId": "85ee9e0e-8f77-4291-e93f-ca8118d28ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=191.788, Accuracy=86.62%\n",
            "Epoch 2: Loss=61.260, Accuracy=96.04%\n",
            "Epoch 3: Loss=45.383, Accuracy=97.00%\n",
            "Epoch 4: Loss=37.984, Accuracy=97.42%\n",
            "Epoch 5: Loss=30.531, Accuracy=97.91%\n",
            "Epoch 6: Loss=28.656, Accuracy=98.09%\n",
            "Epoch 7: Loss=27.331, Accuracy=98.07%\n",
            "Epoch 8: Loss=23.815, Accuracy=98.34%\n",
            "Epoch 9: Loss=25.420, Accuracy=98.25%\n",
            "Epoch 10: Loss=21.541, Accuracy=98.53%\n",
            "Epoch 11: Loss=19.723, Accuracy=98.64%\n",
            "Epoch 12: Loss=19.508, Accuracy=98.62%\n",
            "Epoch 13: Loss=19.617, Accuracy=98.68%\n",
            "Epoch 14: Loss=17.007, Accuracy=98.88%\n",
            "Epoch 15: Loss=16.003, Accuracy=98.89%\n",
            "Epoch 16: Loss=15.663, Accuracy=98.87%\n",
            "Epoch 17: Loss=15.626, Accuracy=98.85%\n",
            "Epoch 18: Loss=16.107, Accuracy=98.81%\n",
            "Epoch 19: Loss=14.800, Accuracy=98.94%\n",
            "Epoch 20: Loss=13.707, Accuracy=99.04%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VisionTransformer().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for imgs, labels in trainloader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        preds = model(imgs)\n",
        "        loss = loss_fn(preds, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (preds.argmax(1) == labels).sum().item()\n",
        "\n",
        "    acc = 100. * total_correct / len(trainset)\n",
        "    print(f\"Epoch {epoch+1}: Loss={total_loss:.3f}, Accuracy={acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obz2BFSQJz-V",
        "outputId": "6adf314f-2321-417b-be42-27b3287deba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 98.03%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in testloader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        preds = model(imgs)\n",
        "        correct += (preds.argmax(1) == labels).sum().item()\n",
        "\n",
        "acc = 100. * correct / len(testset)\n",
        "print(f\"Test Accuracy: {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "KhVkaZLFeyxA",
        "outputId": "9fc01956-e8ea-409f-a925-248471146b17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Trainable Parameters: 1,208,586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9PEdRk9iRhX"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}